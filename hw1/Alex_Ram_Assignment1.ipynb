{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1 \n",
    "#### CS395: Deep learning\n",
    "#### 2/26/18\n",
    "#### By Alex Ram\n",
    "\n",
    "### The code bellow is used for all three parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "from keras.applications.resnet50 import ResNet50 \n",
    "from keras.preprocessing.image import img_to_array\n",
    "from IPython.display import Image\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code bellow was inspired by this tutorial: https://www.pyimagesearch.com/2017/12/11/image-classification-with-keras-and-deep-learning/\n",
    "#This will take the images and covert it to an array, also the labels\n",
    "#path to the pictures\n",
    "PATH = \"../../png/\"\n",
    "trainData = []\n",
    "trainLabels = []\n",
    "\n",
    "testData = []\n",
    "testLabels = []\n",
    " \n",
    "# grab the image paths and randomly shuffle them\n",
    "imagePaths = open(PATH + \"filelist.txt\").readlines()\n",
    "\n",
    "def convertImageToArray(imagePath):\n",
    "    image = cv2.imread(PATH + imagePath.strip())\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = preprocess_input(image)\n",
    "    return image\n",
    "\n",
    "#len(imagePaths)\n",
    "for i in range(160):\n",
    "    if i % 80 < 60:\n",
    "        #image coverted to array and appended to data set\n",
    "        trainData.append(convertImageToArray(imagePaths[i]))\n",
    "        \n",
    "        #labels of images appended to labels dataset\n",
    "        trainLabels.append(imagePaths[i].split(\"/\"))\n",
    "        trainLabels[-1][1] = trainLabels[-1][1].strip()\n",
    "    else:\n",
    "        testData.append(convertImageToArray(imagePaths[i]))\n",
    "        \n",
    "        testLabels.append(imagePaths[i].split(\"/\"))\n",
    "        testLabels[-1][1] = testLabels[-1][1].strip()\n",
    "        \n",
    "#covert label lists to numpy arrays to use later\n",
    "trainLabels = np.array(trainLabels)\n",
    "testLabels = np.array(testLabels)\n",
    "#get a set of just each type of lable\n",
    "uniqueLabels = set(trainLabels[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:\n",
    "### Using a pretrained network on imageNet to classify the sketches data set by training a classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model found here: https://keras.io/applications/#resnet50\n",
    "model = ResNet50(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFeatures = []\n",
    "testFeatures = []\n",
    "\n",
    "#feature extraction from the image arrays using the resnet50 model\n",
    "for i in range(len(trainData)):\n",
    "    trainFeatures.append(model.predict(trainData[i]))\n",
    "    \n",
    "for i in range(len(testData)):\n",
    "    testFeatures.append(model.predict(testData[i]))\n",
    "\n",
    "#squeeze 'em\n",
    "trainFeatures = np.squeeze(trainFeatures)\n",
    "testFeatures = np.squeeze(testFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 40 samples\n",
      "Epoch 1/20\n",
      "120/120 [==============================] - 0s 4ms/step - loss: 0.4401 - acc: 0.7750 - val_loss: 0.2394 - val_acc: 0.9500\n",
      "Epoch 2/20\n",
      "120/120 [==============================] - 0s 432us/step - loss: 0.1587 - acc: 0.9750 - val_loss: 0.2246 - val_acc: 0.9000\n",
      "Epoch 3/20\n",
      "120/120 [==============================] - 0s 347us/step - loss: 0.1406 - acc: 0.9583 - val_loss: 0.1628 - val_acc: 0.9500\n",
      "Epoch 4/20\n",
      "120/120 [==============================] - 0s 369us/step - loss: 0.0870 - acc: 0.9750 - val_loss: 0.1666 - val_acc: 0.9500\n",
      "Epoch 5/20\n",
      "120/120 [==============================] - 0s 345us/step - loss: 0.0754 - acc: 0.9833 - val_loss: 0.1446 - val_acc: 0.9500\n",
      "Epoch 6/20\n",
      "120/120 [==============================] - 0s 366us/step - loss: 0.0511 - acc: 0.9917 - val_loss: 0.1313 - val_acc: 0.9500\n",
      "Epoch 7/20\n",
      "120/120 [==============================] - 0s 362us/step - loss: 0.0446 - acc: 0.9917 - val_loss: 0.1473 - val_acc: 0.9500\n",
      "Epoch 8/20\n",
      "120/120 [==============================] - 0s 402us/step - loss: 0.0385 - acc: 0.9833 - val_loss: 0.1103 - val_acc: 0.9500\n",
      "Epoch 9/20\n",
      "120/120 [==============================] - 0s 371us/step - loss: 0.0219 - acc: 0.9917 - val_loss: 0.1051 - val_acc: 0.9500\n",
      "Epoch 10/20\n",
      "120/120 [==============================] - 0s 332us/step - loss: 0.0246 - acc: 1.0000 - val_loss: 0.1360 - val_acc: 0.9500\n",
      "Epoch 11/20\n",
      "120/120 [==============================] - 0s 369us/step - loss: 0.0116 - acc: 1.0000 - val_loss: 0.1229 - val_acc: 0.9500\n",
      "Epoch 12/20\n",
      "120/120 [==============================] - 0s 410us/step - loss: 0.0100 - acc: 1.0000 - val_loss: 0.1082 - val_acc: 0.9500\n",
      "Epoch 13/20\n",
      "120/120 [==============================] - 0s 323us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 0.1457 - val_acc: 0.9500\n",
      "Epoch 14/20\n",
      "120/120 [==============================] - 0s 340us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 0.2025 - val_acc: 0.9500\n",
      "Epoch 15/20\n",
      "120/120 [==============================] - 0s 347us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.1644 - val_acc: 0.9500\n",
      "Epoch 16/20\n",
      "120/120 [==============================] - 0s 344us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.1307 - val_acc: 0.9500\n",
      "Epoch 17/20\n",
      "120/120 [==============================] - 0s 362us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.1762 - val_acc: 0.9500\n",
      "Epoch 18/20\n",
      "120/120 [==============================] - 0s 344us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.1778 - val_acc: 0.9500\n",
      "Epoch 19/20\n",
      "120/120 [==============================] - 0s 360us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.1849 - val_acc: 0.9500\n",
      "Epoch 20/20\n",
      "120/120 [==============================] - 0s 340us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.1733 - val_acc: 0.9500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f25f4ecee10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code bellow inspired by https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/\n",
    "\n",
    "#Creates the classifier\n",
    "def createClassifier():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(2, input_dim=2048, activation='softmax'))\n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# encode class values as integers\n",
    "def convertToDummy(labels):\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(labels[:,0])\n",
    "    encoded_Y = encoder.transform(labels[:,0])\n",
    "    # convert integers to dummy variables (i.e. one hot encoded)\n",
    "    dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "    return dummy_y\n",
    "\n",
    "trainDummyLabels = convertToDummy(trainLabels)\n",
    "testDummyLabels = convertToDummy(testLabels)\n",
    "\n",
    "estimator = createClassifier()\n",
    "estimator.fit(trainFeatures, trainDummyLabels, validation_data=(testFeatures, testDummyLabels), epochs=20, batch_size=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:\n",
    "### Allowing the entire pretrained network to be trained on the sketches data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm taking the resnet50 model defined in the previous part and adjustiong to have the classifier added to it.\n",
    "#This will allow the entire network to be trained in one go,\n",
    "\n",
    "x = model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "modelWithClassifier = Model(inputs=model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120 samples, validate on 40 samples\n",
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "modelWithClassifier.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "trainData = np.squeeze(np.array(trainData))\n",
    "testData = np.squeeze(np.array(testData))\n",
    "\n",
    "modelWithClassifier.fit(trainData, trainDummyLabels, validation_data=(testData, testDummyLabels), epochs=20, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3:\n",
    "### Initializing the network with _____ and training from scratch on the sketches data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
